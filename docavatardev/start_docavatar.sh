#!/bin/bash
# DocAvatar å¼€å‘ä¸“ç”¨ä¸€é”®å¯åŠ¨è„šæœ¬ï¼ˆä¸æ”¹åŠ¨åŸé¡¹ç›®ä¸ dotsocr.shï¼‰

set -euo pipefail

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PROJECT_ROOT="$( dirname "$SCRIPT_DIR" )"

GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[0;33m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${BLUE}$*${NC}"; }
log_ok()   { echo -e "${GREEN}$*${NC}"; }
log_warn() { echo -e "${YELLOW}$*${NC}"; }
log_err()  { echo -e "${RED}$*${NC}"; }

log_info "ğŸ”§ åˆå§‹åŒ–ç¯å¢ƒ..."

# 1) é¢„æ¸…ç†ï¼šåœæ­¢åŸé¡¹ç›®æœåŠ¡ï¼ˆä¸æ”¹åŠ¨åŸè„šæœ¬é€»è¾‘ï¼Œä»…è°ƒç”¨å…¶ stopï¼‰
if [ -f "$PROJECT_ROOT/dotsocr.sh" ]; then
  log_info "ğŸ§¹ æ¸…ç†åŸé¡¹ç›®æœåŠ¡: ./dotsocr.sh stop"
  bash -c "cd '$PROJECT_ROOT' && ./dotsocr.sh stop" || true
fi

# ä¹Ÿæ¸…ç†å¯èƒ½æ®‹ç•™çš„å¼€å‘ç‰ˆç•Œé¢ä¸ API
pkill -f "docavatar_gradio.py" 2>/dev/null || true
pkill -f "dotsocr/api/server.py" 2>/dev/null || true

# 2) åŠ è½½ conda ç¯å¢ƒï¼šdocavatarï¼ˆä¸¥æ ¼ä½¿ç”¨æŒ‡å®šç¯å¢ƒï¼‰
if [ -f "/home/long/miniconda3/etc/profile.d/conda.sh" ]; then
  # shellcheck disable=SC1091
  source /home/long/miniconda3/etc/profile.d/conda.sh
else
  log_err "æœªæ‰¾åˆ° conda.shï¼Œè¯·ç¡®è®¤ Miniconda/Anaconda å®‰è£…è·¯å¾„ã€‚"
  exit 1
fi

log_info "ğŸ“¦ æ¿€æ´»ç¯å¢ƒ: conda activate docavatar"
conda activate docavatar || { log_err "æ¿€æ´» docavatar å¤±è´¥"; exit 1; }

# æ‰“å°å½“å‰ç¯å¢ƒä¿¡æ¯
PY_BIN=$(python -c 'import sys; print(sys.executable)')
CONDA_ENV=${CONDA_DEFAULT_ENV:-unknown}
log_info "ğŸ” å½“å‰ç¯å¢ƒ: ${CONDA_ENV} (${PY_BIN})"
python - <<'PY'
try:
    import torch
    ver = getattr(torch, "__version__", "?")
    has_cuda = bool(getattr(torch, "cuda", None) and torch.cuda.is_available())
    cuda_ver = getattr(getattr(torch, "version", None), "cuda", None)
    print(f"[torch] {ver} | cuda_available={has_cuda} | cuda={cuda_ver}")
except Exception as e:
    print(f"[torch] ERR: {e}")
try:
    import vllm  # type: ignore
    print("[vllm] OK")
except Exception as e:
    print(f"[vllm] ERR: {e}")
PY

# 3) ç»Ÿä¸€ç¯å¢ƒæ ¡éªŒï¼ˆä¸¥æ ¼ç‰ˆæœ¬ï¼‰
PY_BIN=$(python -c 'import sys; print(sys.executable)')
CONDA_ENV=${CONDA_DEFAULT_ENV:-unknown}
log_info "ğŸ” å½“å‰ç¯å¢ƒ: ${CONDA_ENV} (${PY_BIN})"

# Torch æ ¡éªŒ
python - <<'PY'
try:
    import torch
    ver = getattr(torch, "__version__", "?")
    ok = ver.startswith("2.7.0")
    has_cuda = bool(getattr(torch, "cuda", None) and torch.cuda.is_available())
    cuda_ver = getattr(getattr(torch, "version", None), "cuda", None)
    print(f"[torch] {ver} | ok={ok} | cuda_available={has_cuda} | cuda={cuda_ver}")
except Exception as e:
    print(f"[torch] ERR: {e}")
PY

# 4) è§£ææƒé‡è·¯å¾„ï¼ˆæ”¯æŒä¼ å…¥çˆ¶ç›®å½•ï¼Œä¾‹å¦‚ .../weights ä¼šè‡ªåŠ¨ä½¿ç”¨å…¶ä¸­çš„ DotsOCR å­ç›®å½•ï¼‰
HF_INCOMING_PATH="${HF_MODEL_PATH:-/home/long/mnt/d/AIPJ/016DocAvatar/dotsocr/weights}"

# è‹¥ä¼ å…¥çš„æ˜¯çˆ¶ç›®å½•ä¸”å­˜åœ¨ DotsOCR å­ç›®å½•ï¼Œåˆ™è‡ªåŠ¨ä½¿ç”¨è¯¥å­ç›®å½•
if [ -d "$HF_INCOMING_PATH/DotsOCR" ]; then
  HF_MODEL_PATH="$HF_INCOMING_PATH/DotsOCR"
else
  HF_MODEL_PATH="$HF_INCOMING_PATH"
fi

MODEL_DIR_NAME="$(basename "$HF_MODEL_PATH")"
MODEL_PARENT_DIR="$(dirname "$HF_MODEL_PATH")"

if [ ! -d "$HF_MODEL_PATH" ]; then
  log_warn "æœªæ‰¾åˆ°æƒé‡ç›®å½•: $HF_MODEL_PATH"
  log_warn "è¯·è®¾ç½®ç¯å¢ƒå˜é‡ HF_MODEL_PATH æŒ‡å‘å®é™…çš„æƒé‡ç›®å½•ï¼ˆç›®å½•åä¸èƒ½åŒ…å«ç‚¹å·ï¼‰"
else
  log_info "ä½¿ç”¨æƒé‡ç›®å½•: $HF_MODEL_PATH"
fi

# 5) è®¾ç½® PYTHONPATHï¼ˆå¯¹é½å®˜æ–¹ï¼šåŠ å…¥æƒé‡çˆ¶ç›®å½•ï¼›åŒæ—¶åŠ å…¥é¡¹ç›®ä¾èµ–ï¼‰
export PYTHONPATH="$MODEL_PARENT_DIR:$PROJECT_ROOT/dotsocr:$PROJECT_ROOT/dots_ocr_plugin:$PYTHONPATH"
log_info "PYTHONPATH å·²è®¾ç½®: $PYTHONPATH"
export DOTS_WEIGHTS_DIR="$HF_MODEL_PATH"
log_info "DOTS_WEIGHTS_DIR=$DOTS_WEIGHTS_DIR"
# å¢å¤§è¯»å–è¶…æ—¶ï¼Œé¿å…å¤§æ–‡ä»¶/æ…¢è®¾å¤‡å¯¼è‡´ API è¶…æ—¶
export DOTS_READ_TIMEOUT_SECS=${DOTS_READ_TIMEOUT_SECS:-1800}
log_info "DOTS_READ_TIMEOUT_SECS=$DOTS_READ_TIMEOUT_SECS"

# 6) æ„å»º flash-attnï¼ˆå¦‚æœªå®‰è£…ï¼ŒåŒæ­¥æ‰§è¡Œï¼Œè¾“å‡ºæ—¥å¿—åˆ° flashattn_build.logï¼‰
log_info "âš™ï¸ æ£€æŸ¥/æ„å»º flash-attnï¼ˆHF åŠ é€Ÿï¼‰..."
python - <<'PY'
import importlib.util, os, shutil, sys, subprocess

def which(x):
    return shutil.which(x) is not None

has_fa = importlib.util.find_spec('flash_attn') is not None
if has_fa:
    print('[flash-attn] already installed')
    sys.exit(0)

# å‘ç° CUDA_HOME
cuda_home = os.environ.get('CUDA_HOME')
if not cuda_home:
    cands = ['/usr/local/cuda', '/usr/local/cuda-12.8', '/usr/lib/cuda']
    nvcc = shutil.which('nvcc')
    if nvcc:
        cands.insert(0, os.path.dirname(os.path.dirname(nvcc)))
    for c in cands:
        if os.path.isdir(c):
            cuda_home = c
            break
if cuda_home:
    os.environ['CUDA_HOME'] = cuda_home
    print('[flash-attn] CUDA_HOME=', cuda_home)
else:
    print('[flash-attn] WARN: CUDA_HOME not found; build may fail')

log = os.path.join(os.getcwd(), 'flashattn_build.log')
cmds = [
    [sys.executable, '-m', 'pip', 'install', '-U', 'pip', 'setuptools', 'wheel', 'packaging', 'cmake', 'ninja'],
]
# ä¾æ¬¡å°è¯•å¤šä¸ªç‰ˆæœ¬
for ver in ['2.6.3', '2.7.4.post1', '2.7.2', '2.6.1']:
    cmds.append([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '--default-timeout', '1800', f'flash-attn=={ver}', '-v'])

with open(log, 'w') as f:
    rc = 0
    for c in cmds:
        f.write('$ ' + ' '.join(c) + '\n')
        f.flush()
        p = subprocess.run(c, stdout=f, stderr=subprocess.STDOUT)
        rc = p.returncode
        if rc != 0:
            f.write(f'cmd failed with code {rc}\n')
        # æ£€æŸ¥æ˜¯å¦å·²è£…ä¸Š
        if importlib.util.find_spec('flash_attn') is not None:
            f.write('flash-attn import OK\n')
            print('[flash-attn] build OK')
            sys.exit(0)

print('[flash-attn] build FAILED, details in flashattn_build.log')
sys.exit(0)
PY

# 6) å®‰è£…/æ ¡éªŒ vLLMï¼ˆä»…æ£€æµ‹ï¼Œä¸è‡ªåŠ¨æ›´æ¢ç¯å¢ƒï¼‰
python - <<'PY'
import importlib.util
print("[vllm] installed=", importlib.util.find_spec("vllm") is not None)
PY

# 7) æ³¨å…¥ vLLM CLI å¯¹æ¨¡å‹å¯¼å…¥ï¼ˆæŒ‰å®˜æ–¹è¦æ±‚ï¼Œä½¿ç”¨è‡ªå®šä¹‰ç›®å½•åï¼‰
VLLM_BIN="$(which vllm || true)"
if [ -n "$VLLM_BIN" ]; then
  if ! grep -q "modeling_dots_ocr_vllm" "$VLLM_BIN" 2>/dev/null; then
    log_info "æ³¨å…¥ vLLM CLI å¯¼å…¥: from ${MODEL_DIR_NAME} import modeling_dots_ocr_vllm"
    sed -i "/^from vllm\\.entrypoints\\.cli\\.main import main$/a\\
from ${MODEL_DIR_NAME} import modeling_dots_ocr_vllm" "$VLLM_BIN" || true
  else
    log_info "å·²å­˜åœ¨ vLLM CLI æ³¨å…¥ï¼Œè·³è¿‡"
  fi
else
  log_warn "æœªæ‰¾åˆ° vllm å¯æ‰§è¡Œæ–‡ä»¶ï¼ˆwhich vllm ä¸ºç©ºï¼‰ï¼Œè¯·å…ˆåœ¨ docavatar ç¯å¢ƒå®‰è£… vllm==0.9.1"
fi

# 8) å¯åŠ¨ vLLMï¼ˆä¸¥æ ¼å®˜æ–¹æ–¹å¼ï¼Œä½¿ç”¨ vllm serveï¼‰
log_info "ğŸš€ å¯åŠ¨ vLLM æœåŠ¡å™¨ (vllm serve)..."
cd "$PROJECT_ROOT"
if [ -n "$VLLM_BIN" ]; then
  # ä»…å½“ flash-attn ä¸å¯ç”¨æˆ– GPU ä¸æ”¯æŒæ—¶æ‰åˆ‡æ¢åˆ° SDPA
  CHECK_FA=$(python - <<'PY'
import importlib.util, torch
fa_ok = importlib.util.find_spec('flash_attn') is not None
gpu_ok = torch.cuda.is_available()
print('OK' if (fa_ok and gpu_ok) else 'NO')
PY
  )
  if echo "$CHECK_FA" | grep -q OK; then
    unset VLLM_ATTENTION_BACKEND
    unset VLLM_USE_FLASH_ATTENTION
    log_info "æ³¨æ„åŠ›å®ç°ï¼šä¼˜å…ˆä½¿ç”¨ flash-attnï¼ˆæ£€æµ‹åˆ°å¯ç”¨ï¼‰"
  else
    export VLLM_ATTENTION_BACKEND=SDPA
    export VLLM_USE_FLASH_ATTENTION=0
    log_warn "flash-attn ä¸å¯ç”¨æˆ– GPU ä¸æ”¯æŒï¼Œé™çº§åˆ° SDPA"
  fi
  nohup bash -lc "CUDA_VISIBLE_DEVICES=0 vllm serve '$HF_MODEL_PATH' \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.75 \
    --max-model-len 16384 \
    --chat-template-content-format string \
    --served-model-name model \
    --trust-remote-code" > vllm.log 2>&1 &
  VLLM_PID=$!
  echo $VLLM_PID > vllm.pid
  log_ok "vLLM å·²å¯åŠ¨ (PID: $VLLM_PID) Â· æ¨¡å‹: $HF_MODEL_PATH"
else
  log_err "vLLM æœªå®‰è£…æˆ–æœªæ‰¾åˆ°ã€‚è¯·å…ˆåœ¨ docavatar ç¯å¢ƒæ‰§è¡Œ: pip install -i https://pypi.org/simple --no-cache-dir --default-timeout 600 vllm==0.9.1"
fi

# 9) å¯åŠ¨ API æœåŠ¡ï¼ˆ8080ï¼Œåå°ï¼Œé»˜è®¤ä½¿ç”¨ HFï¼›vLLM åå°åŠ è½½ä¸­å¯éšæ—¶åˆ‡æ¢ï¼‰
log_info "ğŸŒ å¯åŠ¨ API æœåŠ¡ (FastAPI on 8080, mode=hf é»˜è®¤ï¼›å¯éšæ—¶åˆ‡æ¢ vllm/hf/cpu)..."
export DOTS_MODE=hf
export DOTS_VLLM_IP=127.0.0.1
export DOTS_VLLM_PORT=8000
export DOTS_MODEL_NAME=model
# é™åˆ¶å•æ¬¡ç”Ÿæˆ token ä¸Šé™ï¼Œé¿å…è¶…è¿‡ä¸Šä¸‹æ–‡æŠ¥ 400ï¼ˆå¯¹ç”¨æˆ·é€æ˜ï¼Œå¯æŒ‰éœ€å†è°ƒï¼‰
export DOTS_MAX_COMPLETION_TOKENS=${DOTS_MAX_COMPLETION_TOKENS:-1024}
# æ§åˆ¶æ¯æ¬¡ PDF é¡µé¢å¹¶å‘ï¼ˆvllm ä¾§æ›´ç¨³ï¼‰
export DOTS_NUM_THREAD=${DOTS_NUM_THREAD:-2}
# è‡ªåŠ¨æ£€æµ‹ flash-attn æ˜¯å¦å¯ç”¨ï¼Œä¸å¯ç”¨åˆ™é€€å› sdpa ä»¥é¿å…æ— è°“çš„ fallback æ—¥å¿—
HAS_FA=$(python - <<'PY'
import importlib.util
print('1' if importlib.util.find_spec('flash_attn') is not None else '0')
PY
)
if [ "$HAS_FA" = "1" ]; then
  export DOTS_ATTN_IMPL=flash_attention_2
  log_info "æ³¨æ„åŠ›å®ç°: flash_attention_2 (flash-attn å·²å®‰è£…)"
else
  export DOTS_ATTN_IMPL=sdpa
  log_warn "æœªæ£€æµ‹åˆ° flash-attnï¼Œå°†ä½¿ç”¨ sdpaï¼ˆå¯ç”¨ GPU ä»ä¼šåŠ é€Ÿï¼›æ—¥å¿—ä¸å†æç¤º fallbackï¼‰"
fi
nohup python dotsocr/api/server.py > api.log 2>&1 &
API_PID=$!
echo $API_PID > api.pid
sleep 2
if curl -s http://127.0.0.1:8080/health > /dev/null 2>&1; then
  log_ok "API å°±ç»ª (http://127.0.0.1:8080)"
else
  log_warn "API å¯åŠ¨ä¸­... å¯ç¨åè®¿é—® /health æ£€æŸ¥"
fi

# 10) å¯åŠ¨æ–°çš„ Gradio ç•Œé¢ï¼ˆåå°ï¼‰
# å¹¶åŒæ­¥å¯åŠ¨ automindmap (npm start) ä»¥é¿å…é¦–æ¬¡ç‚¹å‡»å»¶è¿Ÿ
export AUTOMINDMAP_PORT=${AUTOMINDMAP_PORT:-5173}
# å›ºå®šç«¯å£ç­–ç•¥ï¼šå†…éƒ¨ Gradio ä»ç›‘å¬ 7860ï¼›å¯¹å¤–ç»Ÿä¸€è½¬å‘åˆ° 10222
export GRADIO_INTERNAL_PORT=${GRADIO_INTERNAL_PORT:-7860}
export GRADIO_PUBLIC_PORT=${GRADIO_PUBLIC_PORT:-10222}
# ä¾› gradio è¯»å–çš„ç¯å¢ƒå˜é‡ï¼ˆå†…éƒ¨å®é™…ç›‘å¬ç«¯å£ï¼‰
export GRADIO_PORT="$GRADIO_INTERNAL_PORT"
export GRADIO_SERVER_PORT="$GRADIO_INTERNAL_PORT"
# ç»‘å®šåœ°å€ç”¨äº dev server ç›‘å¬ï¼Œå…¬å¼€è®¿é—®ä½¿ç”¨ PUBLIC_HOSTï¼ˆä¾›æµè§ˆå™¨/iframe è®¿é—®ï¼‰
export AUTOMINDMAP_BIND=${AUTOMINDMAP_BIND:-0.0.0.0}
export AUTOMINDMAP_PUBLIC_HOST=${AUTOMINDMAP_PUBLIC_HOST:-localhost}
export AUTOMINDMAP_HOST=${AUTOMINDMAP_HOST:-$AUTOMINDMAP_PUBLIC_HOST}
export AUTOMINDMAP_LEFT_CROP=${AUTOMINDMAP_LEFT_CROP:-520}

if [ -d "$SCRIPT_DIR/automindmap" ]; then
  log_info "ğŸ§© å¯åŠ¨ automindmap (npm start on ${AUTOMINDMAP_HOST}:${AUTOMINDMAP_PORT})..."
  if command -v npm >/dev/null 2>&1; then
    (
      cd "$SCRIPT_DIR/automindmap"
      # å…ˆæ€æ‰å¯èƒ½æ®‹ç•™çš„æ—§è¿›ç¨‹ï¼Œé¿å…ç«¯å£è¢«å ç”¨å¯¼è‡´è‡ªåŠ¨æ¢ç«¯å£
      pkill -f "docavatardev/automindmap/server.js" 2>/dev/null || true
      pkill -f "node .*automindmap/server.js" 2>/dev/null || true
      if [ ! -d node_modules ]; then
        npm ci || npm install
      fi
      HOST="${AUTOMINDMAP_BIND}" PORT="${AUTOMINDMAP_PORT}" nohup npm start > automindmap.log 2>&1 &
      echo $! > automindmap.pid
    )
    log_ok "automindmap å·²å¯åŠ¨ (ç›‘å¬ ${AUTOMINDMAP_BIND}:${AUTOMINDMAP_PORT} Â· è®¿é—® http://${AUTOMINDMAP_PUBLIC_HOST}:${AUTOMINDMAP_PORT})"
  else
    log_warn "æœªæ£€æµ‹åˆ° npmï¼Œå·²è·³è¿‡ automindmap å¯åŠ¨ã€‚è¯·å®‰è£… Node.js åé‡è¯•ã€‚"
  fi
fi
# æ¸…ç†å‰ç«¯ä¸åç«¯ç¼“å­˜ç›®å½•ï¼Œä¿è¯â€œé‡è·‘å³æ–°ç»“æœâ€ï¼›éšåç«‹å³é‡å»ºå¿…è¦ç›®å½•
rm -rf "$PROJECT_ROOT/dotsocr/api/cache" "$PROJECT_ROOT/dotsocr/api/user_md" "$PROJECT_ROOT/output_api" "$PROJECT_ROOT/docavatardev/output/parsed" 2>/dev/null || true
mkdir -p "$PROJECT_ROOT/dotsocr/api/cache" "$PROJECT_ROOT/dotsocr/api/user_md" "$PROJECT_ROOT/output_api" "$PROJECT_ROOT/docavatardev/output/parsed" 2>/dev/null || true

log_info "ğŸ–¼ï¸ å¯åŠ¨ DocAvatar Gradio ç•Œé¢..."
nohup bash -lc "GRADIO_PORT=$GRADIO_INTERNAL_PORT GRADIO_SERVER_PORT=$GRADIO_INTERNAL_PORT python '$SCRIPT_DIR/docavatar_gradio.py'" > "$PROJECT_ROOT/docavatar_gradio.log" 2>&1 &
GRADIO_PID=$!
echo $GRADIO_PID > "$PROJECT_ROOT/docavatar_gradio.pid"

# å¯åŠ¨ 10222 â†’ 7860 çš„æœ¬åœ°è½¬å‘ä»£ç†ï¼ˆHTTP + WebSocketï¼‰
if command -v node >/dev/null 2>&1; then
  pkill -f "docavatardev/forward_10222.js" 2>/dev/null || true
  nohup bash -lc "GRADIO_INTERNAL_PORT=$GRADIO_INTERNAL_PORT GRADIO_PUBLIC_PORT=$GRADIO_PUBLIC_PORT node '$SCRIPT_DIR/forward_10222.js'" > "$PROJECT_ROOT/forward_10222.log" 2>&1 &
  echo $! > "$PROJECT_ROOT/forward_10222.pid"
  log_ok "è½¬å‘å·²å¯ç”¨: http://localhost:${GRADIO_PUBLIC_PORT} -> http://127.0.0.1:${GRADIO_INTERNAL_PORT}"
else
  log_warn "æœªæ£€æµ‹åˆ° nodeï¼Œæ— æ³•å¯ç”¨ç«¯å£è½¬å‘ã€‚è¯·å®‰è£… Node.js æˆ–ä½¿ç”¨ nginx/socat è‡ªè¡Œè½¬å‘ ${GRADIO_PUBLIC_PORT} -> ${GRADIO_INTERNAL_PORT}."
fi

sleep 3
if curl -s http://127.0.0.1:${GRADIO_INTERNAL_PORT} > /dev/null 2>&1; then
  log_ok "Gradio(å†…éƒ¨) å°±ç»ªï¼(http://127.0.0.1:${GRADIO_INTERNAL_PORT})"
  log_info "å¯¹å¤–è®¿é—®è¯·ä½¿ç”¨: http://localhost:${GRADIO_PUBLIC_PORT}"
  log_info "Automindmap ç›®æ ‡: http://${AUTOMINDMAP_PUBLIC_HOST}:${AUTOMINDMAP_PORT}"
else
  log_warn "Gradio ç•Œé¢å¯åŠ¨ä¸­..."
fi

# 11) æ‰“å¼€æµè§ˆå™¨
sleep 5  # ç»™Gradioæ›´å¤šæ—¶é—´å®Œå…¨å¯åŠ¨
log_info "æ£€æµ‹æµè§ˆå™¨å¹¶æ‰“å¼€ç•Œé¢..."

# WSLç¯å¢ƒç‰¹æ®Šå¤„ç†
if grep -q Microsoft /proc/version 2>/dev/null; then
  log_info "æ£€æµ‹åˆ°WSLç¯å¢ƒï¼Œå°è¯•ä½¿ç”¨Windowsæµè§ˆå™¨..."
  # å°è¯•é€šè¿‡WSLæ‰“å¼€Windowsæµè§ˆå™¨
  if command -v cmd.exe > /dev/null 2>&1; then
    cmd.exe /c start "http://localhost:${GRADIO_PUBLIC_PORT}" > /dev/null 2>&1 &
    log_ok "å·²é€šè¿‡Windowsæµè§ˆå™¨æ‰“å¼€"
  elif command -v powershell.exe > /dev/null 2>&1; then
    powershell.exe -Command "Start-Process 'http://localhost:${GRADIO_PUBLIC_PORT}'" > /dev/null 2>&1 &
    log_ok "å·²é€šè¿‡PowerShellæ‰“å¼€"
  else
    log_warn "WSLç¯å¢ƒä¸‹æœªæ‰¾åˆ°Windowså‘½ä»¤ï¼Œè¯·æ‰‹åŠ¨è®¿é—®: http://localhost:${GRADIO_PUBLIC_PORT}"
  fi
elif command -v xdg-open > /dev/null 2>&1; then
  log_info "æ­£åœ¨æ‰“å¼€æµè§ˆå™¨..."
  xdg-open "http://localhost:${GRADIO_PUBLIC_PORT}" > /dev/null 2>&1 &
  log_ok "å·²é€šè¿‡xdg-openæ‰“å¼€"
elif command -v firefox > /dev/null 2>&1; then
  log_info "æ­£åœ¨ç”¨ Firefox æ‰“å¼€æµè§ˆå™¨..."
  firefox "http://localhost:${GRADIO_PUBLIC_PORT}" > /dev/null 2>&1 &
  log_ok "å·²é€šè¿‡Firefoxæ‰“å¼€"
elif command -v google-chrome > /dev/null 2>&1; then
  log_info "æ­£åœ¨ç”¨ Chrome æ‰“å¼€æµè§ˆå™¨..."
  google-chrome "http://localhost:${GRADIO_PUBLIC_PORT}" > /dev/null 2>&1 &
  log_ok "å·²é€šè¿‡Chromeæ‰“å¼€"
else
  log_warn "æœªæ‰¾åˆ°æµè§ˆå™¨ï¼Œè¯·æ‰‹åŠ¨è®¿é—®: http://localhost:${GRADIO_PUBLIC_PORT}"
fi

echo ""
echo "========================================="
log_ok "ğŸ‰ DocAvatar å¼€å‘ç¯å¢ƒå¯åŠ¨æˆåŠŸï¼"
echo "========================================="
echo -e "${BLUE}ğŸ“± ç•Œé¢åœ°å€(å¯¹å¤–):${NC} http://localhost:${GRADIO_PUBLIC_PORT}"
echo -e "${BLUE}ğŸ“¦ å†…éƒ¨ Gradio ç«¯å£:${NC} http://127.0.0.1:${GRADIO_INTERNAL_PORT}"
echo -e "${BLUE}ğŸš€ vLLM API:${NC} http://localhost:8000"
echo -e "${BLUE}ğŸ”— DocAvatar API:${NC} http://localhost:8080"
echo ""
echo -e "${YELLOW}æç¤º:${NC} é»˜è®¤ API æ¨¡å¼ä¸º hfï¼›å¯é€šè¿‡ query 'mode=vllm|hf|cpu' åˆ‡æ¢ã€‚"
echo -e "${YELLOW}åœæ­¢:${NC} å¯è¿è¡Œ ./dotsocr.sh stop åœæ­¢ vLLMï¼›æˆ– pkill -f docavatar_gradio.py åœæ­¢æ–°ç•Œé¢ã€‚"
echo "========================================="

# 12) åœ¨ docavatar ç¯å¢ƒåå°æ„å»º flash-attnï¼ˆå¦‚æœªå®‰è£…ä¸” GPU å¯ç”¨ï¼‰ï¼Œæ—¥å¿—è¾“å‡ºåˆ° flashattn_build.log
python - <<'PY' > /dev/null 2>&1 || true
import importlib.util, torch, os, subprocess, sys
need = importlib.util.find_spec('flash_attn') is None
gpu_ok = torch.cuda.is_available()
if need and gpu_ok:
    log = os.path.join(os.getcwd(), 'flashattn_build.log')
    cmd = [
        sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '--default-timeout', '1200',
        'flash-attn==2.6.3'
    ]
    with open(log, 'w') as f:
        f.write('Building flash-attn from source...\n')
        f.flush()
        subprocess.Popen(cmd, stdout=f, stderr=subprocess.STDOUT)
PY

# 12) åå°å®æ—¶è·Ÿéšæ—¥å¿—åˆ°ç»ˆç«¯ï¼ˆå¯ Ctrl+C é€€å‡ºï¼‰
log_info "ğŸ“œ è·Ÿéšæ—¥å¿—ï¼švLLM ä¸ APIï¼ˆCtrl+C é€€å‡ºï¼ŒæœåŠ¡ä¸åœæ­¢ï¼‰"
(
  echo "--- tail -f vllm.log ---";
  tail -n 20 -f vllm.log &
  T1=$!
  echo "--- tail -f api.log ---";
  tail -n 20 -f api.log &
  T2=$!
  wait $T1 $T2
) || true


