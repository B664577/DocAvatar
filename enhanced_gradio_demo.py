"""
Enhanced Gradio Demo for DotsOCR with Multiple Inference Modes
æ”¯æŒ vLLMã€HuggingFace å’Œ CPU æ¨ç†æ¨¡å¼çš„ç»Ÿä¸€ç•Œé¢
"""

import gradio as gr
import json
import os
import io
import tempfile
import base64
import uuid
import time
import subprocess
import signal
import atexit
import threading
import sys
from pathlib import Path
from PIL import Image
import requests
import psutil

# Add project paths to sys.path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(project_root, 'dotsocr'))
sys.path.insert(0, os.path.join(project_root, 'dots_ocr_plugin'))

# Import DotsOCR components
try:
    from dots_ocr.utils import dict_promptmode_to_prompt
    from dots_ocr.utils.consts import MIN_PIXELS, MAX_PIXELS
    from dots_ocr.utils.demo_utils.display import read_image
    from dots_ocr.utils.doc_utils import load_images_from_pdf
    from dots_ocr.parser import DotsOCRParser
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure you're in the correct environment and paths are set properly.")
    sys.exit(1)

# Global variables
vllm_process = None
current_mode = "vllm"
parser_instances = {}

# ==================== Service Management ====================

def check_vllm_server(host="127.0.0.1", port=8000, timeout=5):
    """æ£€æŸ¥ vLLM æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get(f"http://{host}:{port}/v1/models", timeout=timeout)
        return response.status_code == 200
    except:
        return False

def start_vllm_server():
    """å¯åŠ¨ vLLM æœåŠ¡å™¨"""
    global vllm_process
    
    if check_vllm_server():
        return "âœ… vLLM æœåŠ¡å™¨å·²è¿è¡Œ"
    
    try:
        # ä½¿ç”¨æˆ‘ä»¬çš„æ’ä»¶åŒ–å¯åŠ¨è„šæœ¬
        startup_script = "/home/long/mnt/d/AIPJ/013DocAvatar/start_vllm_with_plugin.py"
        if not os.path.exists(startup_script):
            return "âŒ å¯åŠ¨è„šæœ¬ä¸å­˜åœ¨"
        
        # æ¿€æ´» conda ç¯å¢ƒå¹¶å¯åŠ¨
        cmd = [
            "bash", "-c", 
            f"source /home/long/miniconda3/etc/profile.d/conda.sh && "
            f"conda activate dots_ocr && "
            f"export PYTHONPATH='/home/long/mnt/d/AIPJ/013DocAvatar/dots_ocr_plugin:$PYTHONPATH' && "
            f"python {startup_script}"
        ]
        
        vllm_process = subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            preexec_fn=os.setsid
        )
        
        # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        for i in range(120):  # ç­‰å¾…æœ€å¤š2åˆ†é’Ÿ
            time.sleep(1)
            if check_vllm_server():
                return "âœ… vLLM æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ"
            if vllm_process.poll() is not None:
                output = vllm_process.stdout.read()
                return f"âŒ vLLM å¯åŠ¨å¤±è´¥: {output}"
        
        return "â³ vLLM æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨å€™..."
        
    except Exception as e:
        return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}"

def stop_vllm_server():
    """åœæ­¢ vLLM æœåŠ¡å™¨"""
    global vllm_process
    
    try:
        if vllm_process:
            os.killpg(os.getpgid(vllm_process.pid), signal.SIGTERM)
            vllm_process = None
        
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„è¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if 'vllm' in proc.info['name'] or any('vllm' in cmd for cmd in proc.info['cmdline']):
                    proc.terminate()
            except:
                pass
        
        return "âœ… vLLM æœåŠ¡å™¨å·²åœæ­¢"
    except Exception as e:
        return f"âŒ åœæ­¢å¤±è´¥: {str(e)}"

def cleanup_on_exit():
    """ç¨‹åºé€€å‡ºæ—¶æ¸…ç†èµ„æº"""
    if vllm_process:
        stop_vllm_server()

atexit.register(cleanup_on_exit)

# ==================== Parser Management ====================

def get_parser(mode="vllm"):
    """è·å–æŒ‡å®šæ¨¡å¼çš„è§£æå™¨å®ä¾‹"""
    global parser_instances
    
    if mode not in parser_instances:
        # æ£€æŸ¥ CUDA å¯ç”¨æ€§
        cuda_available = False
        try:
            import torch
            cuda_available = torch.cuda.is_available()
        except:
            pass
        
        model_path = "/home/long/mnt/d/AIPJ/013DocAvatar/dotsocr/weights/DotsOCR"
        
        if mode == "vllm":
            parser_instances[mode] = DotsOCRParser(
                ip="127.0.0.1",
                port=8000,
                use_hf=False,
                device="cuda" if cuda_available else "cpu",
                min_pixels=MIN_PIXELS,
                max_pixels=MAX_PIXELS
            )
        elif mode == "hf":
            parser_instances[mode] = DotsOCRParser(
                model_path=model_path,
                use_hf=True,
                device="cuda" if cuda_available else "cpu",
                min_pixels=MIN_PIXELS,
                max_pixels=MAX_PIXELS
            )
        elif mode == "cpu":
            parser_instances[mode] = DotsOCRParser(
                model_path=model_path,
                use_hf=True,
                device="cpu",
                min_pixels=MIN_PIXELS,
                max_pixels=MAX_PIXELS
            )
    
    return parser_instances[mode]

# ==================== Core Processing Functions ====================

def process_image(image, prompt_mode, inference_mode, bbox_input=""):
    """å¤„ç†å›¾åƒçš„æ ¸å¿ƒå‡½æ•°"""
    if image is None:
        return None, "è¯·ä¸Šä¼ å›¾åƒ", None
    
    try:
        # è·å–å¯¹åº”æ¨¡å¼çš„è§£æå™¨
        parser = get_parser(inference_mode)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="dots_ocr_")
        session_id = uuid.uuid4().hex[:8]
        
        # å¦‚æœæä¾›äº† bboxï¼Œè§£æå®ƒ
        bbox = None
        if bbox_input.strip():
            try:
                bbox_values = [int(x.strip()) for x in bbox_input.split(',')]
                if len(bbox_values) == 4:
                    bbox = bbox_values
            except:
                pass
        
        # ä¿å­˜è¾“å…¥å›¾åƒ
        input_path = os.path.join(temp_dir, f"input_{session_id}.png")
        image.save(input_path, "PNG")
        
        # æ‰§è¡Œè§£æ
        start_time = time.time()
        
        if bbox:
            # ä½¿ç”¨ bbox æ¨¡å¼
            results = parser.parse_image(
                input_path=input_path,
                filename=f"demo_{session_id}",
                prompt_mode="prompt_grounding_ocr",
                save_dir=temp_dir,
                bbox=bbox
            )
        else:
            # æ­£å¸¸è§£ææ¨¡å¼
            results = parser.parse_image(
                input_path=input_path,
                filename=f"demo_{session_id}",
                prompt_mode=prompt_mode,
                save_dir=temp_dir
            )
        
        processing_time = time.time() - start_time
        
        if not results:
            return None, "è§£æå¤±è´¥ï¼šæ— ç»“æœè¿”å›", None
        
        result = results[0]
        
        # è·å–å¸ƒå±€å›¾åƒ
        layout_image = None
        if 'layout_image_path' in result and os.path.exists(result['layout_image_path']):
            layout_image = Image.open(result['layout_image_path'])
        
        # è·å–è§£æç»“æœ
        layout_info = ""
        if 'layout_info_path' in result and os.path.exists(result['layout_info_path']):
            with open(result['layout_info_path'], 'r', encoding='utf-8') as f:
                layout_data = json.load(f)
                layout_info = json.dumps(layout_data, ensure_ascii=False, indent=2)
        
        status_msg = f"âœ… è§£æå®Œæˆ | æ¨¡å¼: {inference_mode.upper()} | è€—æ—¶: {processing_time:.2f}s"
        
        return layout_image, status_msg, layout_info
        
    except Exception as e:
        return None, f"âŒ è§£æå¤±è´¥: {str(e)}", None

def switch_inference_mode(mode):
    """åˆ‡æ¢æ¨ç†æ¨¡å¼"""
    global current_mode
    current_mode = mode
    
    status_msgs = []
    
    if mode == "vllm":
        if check_vllm_server():
            status_msgs.append("âœ… vLLM æœåŠ¡å™¨è¿è¡Œä¸­")
        else:
            status_msgs.append("âš ï¸ vLLM æœåŠ¡å™¨æœªå¯åŠ¨ï¼Œè¯·ç‚¹å‡»å¯åŠ¨æŒ‰é’®")
    elif mode == "hf":
        status_msgs.append("âœ… åˆ‡æ¢åˆ° HuggingFace æ¨¡å¼")
    elif mode == "cpu":
        status_msgs.append("âœ… åˆ‡æ¢åˆ° CPU æ¨¡å¼")
    
    return f"å½“å‰æ¨¡å¼: {mode.upper()}\n" + "\n".join(status_msgs)

# ==================== Gradio Interface ====================

def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="DotsOCR Enhanced Demo",
        theme=gr.themes.Soft(),
        css="""
        .inference-mode-radio .wrap {
            display: flex !important;
            flex-direction: row !important;
        }
        .inference-mode-radio .wrap > label {
            margin-right: 20px !important;
        }
        .status-box {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            background-color: #f9f9f9;
            margin: 10px 0;
        }
        .control-button {
            margin: 5px;
        }
        """
    ) as demo:
        
        gr.Markdown("""
        # ğŸš€ DotsOCR Enhanced Demo
        
        ## åŠŸèƒ½ç‰¹ç‚¹
        - **å¤šæ¨¡å¼æ¨ç†**: æ”¯æŒ vLLMã€HuggingFaceã€CPU ä¸‰ç§æ¨ç†æ¨¡å¼
        - **ä¸€é”®å¯åŠ¨**: è‡ªåŠ¨ç®¡ç† vLLM æœåŠ¡å™¨
        - **å®æ—¶åˆ‡æ¢**: æ— éœ€é‡å¯å³å¯åˆ‡æ¢æ¨ç†æ¨¡å¼
        - **å®Œæ•´åŠŸèƒ½**: æ”¯æŒæ–‡æ¡£è§£æã€å¸ƒå±€æ£€æµ‹ã€OCR è¯†åˆ«
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ¨ç†æ¨¡å¼é€‰æ‹©
                gr.Markdown("### ğŸ”§ æ¨ç†æ¨¡å¼è®¾ç½®")
                
                inference_mode = gr.Radio(
                    choices=["vllm", "hf", "cpu"],
                    value="vllm",
                    label="æ¨ç†æ¨¡å¼",
                    info="é€‰æ‹©æ¨ç†åç«¯",
                    elem_classes=["inference-mode-radio"]
                )
                
                mode_status = gr.Textbox(
                    value="å½“å‰æ¨¡å¼: VLLM",
                    label="æ¨¡å¼çŠ¶æ€",
                    interactive=False,
                    elem_classes=["status-box"]
                )
                
                # vLLM æœåŠ¡ç®¡ç†
                with gr.Group():
                    gr.Markdown("### ğŸ–¥ï¸ vLLM æœåŠ¡ç®¡ç†")
                    with gr.Row():
                        start_btn = gr.Button("ğŸš€ å¯åŠ¨ vLLM", elem_classes=["control-button"])
                        stop_btn = gr.Button("ğŸ›‘ åœæ­¢ vLLM", elem_classes=["control-button"])
                    
                    server_status = gr.Textbox(
                        label="æœåŠ¡çŠ¶æ€",
                        interactive=False,
                        elem_classes=["status-box"]
                    )
                
                # è§£æè®¾ç½®
                gr.Markdown("### âš™ï¸ è§£æè®¾ç½®")
                
                prompt_mode = gr.Dropdown(
                    choices=list(dict_promptmode_to_prompt.keys()),
                    value="prompt_layout_all_en",
                    label="æç¤ºæ¨¡å¼",
                    info="é€‰æ‹©è§£æä»»åŠ¡ç±»å‹"
                )
                
                bbox_input = gr.Textbox(
                    label="è¾¹ç•Œæ¡† (å¯é€‰)",
                    placeholder="æ ¼å¼: x1,y1,x2,y2 (ä¾‹å¦‚: 100,100,500,400)",
                    info="æŒ‡å®šåŒºåŸŸè§£æï¼Œç•™ç©ºè¡¨ç¤ºå…¨å›¾è§£æ"
                )
            
            with gr.Column(scale=2):
                # å›¾åƒè¾“å…¥åŒºåŸŸ
                gr.Markdown("### ğŸ“· å›¾åƒè¾“å…¥")
                
                with gr.Row():
                    input_image = gr.Image(
                        label="ä¸Šä¼ å›¾åƒ",
                        type="pil",
                        height=400
                    )
                    
                    output_image = gr.Image(
                        label="è§£æç»“æœ",
                        type="pil",
                        height=400
                    )
                
                # å¤„ç†æŒ‰é’®
                process_btn = gr.Button("ğŸ” å¼€å§‹è§£æ", variant="primary", size="lg")
                
                # çŠ¶æ€æ˜¾ç¤º
                processing_status = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    elem_classes=["status-box"]
                )
                
                # ç»“æœå±•ç¤º
                gr.Markdown("### ğŸ“‹ è§£æç»“æœ")
                result_json = gr.Code(
                    label="JSON ç»“æœ",
                    language="json",
                    lines=15
                )
        
        # ==================== Event Handlers ====================
        
        # æ¨ç†æ¨¡å¼åˆ‡æ¢
        inference_mode.change(
            fn=switch_inference_mode,
            inputs=[inference_mode],
            outputs=[mode_status]
        )
        
        # vLLM æœåŠ¡ç®¡ç†
        start_btn.click(
            fn=start_vllm_server,
            outputs=[server_status]
        )
        
        stop_btn.click(
            fn=stop_vllm_server,
            outputs=[server_status]
        )
        
        # å›¾åƒå¤„ç†
        process_btn.click(
            fn=process_image,
            inputs=[input_image, prompt_mode, inference_mode, bbox_input],
            outputs=[output_image, processing_status, result_json]
        )
        
        # åˆå§‹åŒ–æ£€æŸ¥
        def initial_check():
            if check_vllm_server():
                return "âœ… vLLM æœåŠ¡å™¨è¿è¡Œä¸­"
            else:
                return "âš ï¸ vLLM æœåŠ¡å™¨æœªå¯åŠ¨"
        
        demo.load(
            fn=initial_check,
            outputs=[server_status]
        )
    
    return demo

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="DotsOCR Enhanced Gradio Demo")
    parser.add_argument("--port", type=int, default=7860, help="Gradio server port")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Gradio server host")
    parser.add_argument("--share", action="store_true", help="Create public link")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_interface()
    
    print(f"""
ğŸš€ DotsOCR Enhanced Demo å¯åŠ¨ä¸­...

ğŸŒ è®¿é—®åœ°å€: http://localhost:{args.port}
ğŸ“± åŠŸèƒ½ç‰¹ç‚¹:
   - æ”¯æŒ vLLM/HuggingFace/CPU ä¸‰ç§æ¨ç†æ¨¡å¼
   - ä¸€é”®å¯åŠ¨ vLLM æœåŠ¡å™¨
   - å®æ—¶æ¨¡å¼åˆ‡æ¢
   - å®Œæ•´çš„æ–‡æ¡£è§£æåŠŸèƒ½

ğŸ’¡ ä½¿ç”¨æç¤º:
   1. é€‰æ‹©æ¨ç†æ¨¡å¼ (æ¨èå…ˆç”¨ vLLM)
   2. å¦‚ä½¿ç”¨ vLLMï¼Œç‚¹å‡»"å¯åŠ¨ vLLM"æŒ‰é’®
   3. ä¸Šä¼ å›¾åƒå¹¶é€‰æ‹©è§£ææ¨¡å¼
   4. ç‚¹å‡»"å¼€å§‹è§£æ"æŸ¥çœ‹ç»“æœ
""")
    
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        inbrowser=True
    )